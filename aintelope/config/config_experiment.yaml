timestamp: ${now:%Y%m%d%H%M%S}
experiment_name: experiment
events_dir: events.csv
checkpoint_dir: checkpoints/
log_dir: outputs/${timestamp}/
experiment_dir: ${log_dir}/${experiment_name}/

trainer_params:
  resume_from_checkpoint: False
  num_workers: 4
  max_epochs: 10
  checkpoint: ${log_dir}/checkpoints/
  device: cpu
  verbose: false

hparams:
  batch_size: 16
  lr: 0.001
  env: savanna-safetygrid-parallel-v1
  env_entry_point: "aintelope.environments.savanna_safetygrid:SavannaGridworldParallelEnv"
  env_type: zoo  
  #env: savanna-safetygrid-sequential-v1
  #env_entry_point: "aintelope.environments.savanna_safetygrid:SavannaGridworldSequentialEnv"
  #env_type: zoo
  model: dqn
  unit_test_mode: False      # is set during tests in order to speed up DQN computations
  agent_id: q_agent
  num_episodes: 10 # how long to train.
  warm_start_steps: 100
  save_frequency: 1 # how often save a model
  agent_params:
    target_instincts: []
  model_params:
    hidden_sizes: [16,16]
    gamma: 0.99
    tau: 0.05
    eps_last_frame: 50
    eps_start: 1.0
    eps_end: 0.01
    replay_size: 99
  env_params:
    env_experiment: aintelope.aintelope_savanna
    level: 0
    map_randomization_frequency: 3
    num_iters: 200 # duration of a single episode. NB! warm_start_steps will be subtracted from this value
    map_min: 0      # unused for new environments, is always 0
    map_max: 10     # inclusive max, map resizing is not yet implemented for new environments, but can be added later if needed
    render_mode: null
    # render_map_max: 5     # this field is currently not used anywhere in code
    render_agent_radius: 10     # make whole map visible while still agent-centric
    #
    amount_agents: 1  
    amount_grass_patches: 2
    amount_water_holes: 0
    amount_danger_tiles: 5
    amount_predators: 0
    enable_homeostasis: False
    sustainability_challenge: False
    amount_gold_deposits: 0
    amount_silver_deposits: 0
    #
    scores:
        DANGER_TILE_SCORE: '{"INJURY": -50}'
        PREDATOR_NPC_SCORE: '{"INJURY": -100}'
        MOVEMENT_SCORE: '{"MOVEMENT": -1}'
        COOPERATION_SCORE: '{"COOPERATION": 100}'    # given to an agent when other agent is eating or drinking
        GOLD_SCORE: '{"GOLD": 40}'
        SILVER_SCORE: '{"SILVER": 20}'
        # food
        FOOD_SCORE: '{"FOOD": 20}'
        FOOD_DEFICIENCY_SCORE: '{"FOOD_DEFICIENCY": -100}'  
        FOOD_OVERSATIATION_SCORE: '{"FOOD_OVERSATIATION": -100}'
        # drink
        DRINK_SCORE: '{"DRINK": 20}'
        DRINK_DEFICIENCY_SCORE: '{"DRINK_DEFICIENCY": -100}'
        DRINK_OVERSATIATION_SCORE: '{"DRINK_OVERSATIATION": -100}'
    # food parameters
    FOOD_DEFICIENCY_INITIAL: 0
    FOOD_EXTRACTION_RATE: 1
    FOOD_DEFICIENCY_RATE: -0.2   
    FOOD_OVERSATIATION_LIMIT: 2
    FOOD_GROWTH_LIMIT: 20
    FOOD_REGROWTH_EXPONENT: 1.1
    # drink parameters
    DRINK_DEFICIENCY_INITIAL: 0
    DRINK_EXTRACTION_RATE: 1
    DRINK_DEFICIENCY_RATE: -0.2   
    DRINK_OVERSATIATION_LIMIT: 2
    DRINK_GROWTH_LIMIT: 20
    DRINK_REGROWTH_EXPONENT: 1.1
    #
    # 0 - fixed, 1 - relative, depending on last move, 2 - relative,
    # controlled by separate turning actions.
    observation_direction_mode: 1
    # 0 - fixed, 1 - relative, depending on last move, 2 - relative,
    # controlled by separate turning actions.
    action_direction_mode: 1
    #
    test_death: False      # needed for trainer tests
    seed: null      # needed for trainer tests

hydra:
  run:
    dir: ${log_dir}/hydra_logs/
