import functools
import random
import typing as typ
from pprint import pprint

import numpy as np
import pygame
from gym.spaces import Box, Discrete
from gym.utils import seeding
from pettingzoo import AECEnv
from pettingzoo.utils import agent_selector, wrappers

PositionFloat = np.float32

NUM_ITERS = 500  # duration of the game
MAP_MIN, MAP_MAX = 0, 100
CYCLIC_BOUNDARIES = True
# TODO: NUM_AGENTS
AMOUNT_AGENTS = 1  # for now only one agent
AMOUNT_GRASS = 2
ACTION_MAP = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]], dtype=PositionFloat)


class RenderSettings:
    def __init__(self, metadata):
        prefix = "render_"
        settings = {
            (k.lstrip(prefix), v)
            for k, v in metadata.items()
            if k.startswith(prefix)
        }
        self.__dict__.update(settings)


class RenderState:
    def __init__(self, settings):
        canvas = pygame.Surface((settings.window_size, settings.window_size))
        self.canvas = canvas
        self.settings = settings

    def render(self, agents_state, grass):
        window_size = self.settings.window_size
        canvas = self.canvas

        canvas.fill((255, 255, 255))
        scale = window_size / MAP_MAX

        screen_m = np.identity(2, dtype=PositionFloat) * scale

        def project(p):
            return np.matmul(p, screen_m).astype(np.int32)

        for gr in grass.reshape((2, -1)):
            p = project(gr)
            pygame.draw.circle(
                canvas,
                self.settings.grass_color,
                p,
                scale * self.settings.grass_radius,
            )

        for agent, agent_pos in agents_state.items():
            assert len(agent_pos) == 2, agent_pos
            # TODO: render agent name as text
            p = project(agent_pos)
            pygame.draw.circle(
                canvas,
                self.settings.agent_color,
                p,
                scale * self.settings.agent_radius,
            )


class HumanRenderState:
    def __init__(self, settings):

        self.fps = settings.fps

        window_size = settings.window_size

        pygame.init()
        pygame.display.init()
        self.window = pygame.display.set_mode((window_size, window_size))
        self.clock = pygame.time.Clock()

    def render(self, render_state):
        # The following line copies our drawings from `canvas` to the visible window
        self.window.blit(render_state.canvas, render_state.canvas.get_rect())
        pygame.event.pump()
        pygame.display.update()

        # We need to ensure that human-rendering occurs at the predefined framerate.
        # The following line will automatically add a delay to keep the framerate stable.
        self.clock.tick(self.fps)


class RawEnv(AECEnv):

    metadata = {
        "name": "savanna_v1",
        "render_fps": 15,
        "render_agent_radius": 5,
        "render_agent_color": (200, 50, 0),
        "render_grass_radius": 5,
        "render_grass_color": (20, 200, 0),
        "render_modes": ("human",),
        "render_window_size": 512,
    }

    def __init__(self):
        self.possible_agents = [f"player_{r}" for r in range(AMOUNT_AGENTS)]
        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(AMOUNT_AGENTS)))
        )

        self._action_spaces = {
            agent: Discrete(4) for agent in self.possible_agents
        }  # agents can walk in 4 directions
        self._observation_spaces = {
            agent: Box(
                MAP_MIN, MAP_MAX, shape=(2 * (AMOUNT_AGENTS + AMOUNT_GRASS),)
            )
            for agent in self.possible_agents
        }

        render_settings = RenderSettings(self.metadata)
        self.render_state = RenderState(render_settings)
        self.human_render_state = None
        self.seed()

    @functools.lru_cache(maxsize=None)
    def observation_space(self, agent: str):
        return self._observation_spaces[agent]

    @functools.lru_cache(maxsize=None)
    def action_space(self, agent: str):
        return self._action_spaces[agent]

    def seed(self, seed: typ.Optional[int] = None) -> None:
        self.np_random, seed = seeding.np_random(seed)

    def observe(self, agent: str):
        """Return observation of given agent."""
        return np.array(self.observations[agent])

    def render(self, mode="human"):
        """Render the environment."""

        self.render_state.render(self.state, self.grass)

        if mode == "human":
            if not self.human_render_state:
                self.human_render_state = HumanRenderState(
                    self.render_state.settings
                )
            self.human_render_state.render(self.render_state)
        else:  # rgb_array
            return np.transpose(
                np.array(pygame.surfarray.pixels3d(self.render_state.canvas)),
                axes=(1, 0, 2),
            )

    def close(self):
        """Release any graphical display, subprocesses, network connections
        or any other environment data which should not be kept around after
        the user is no longer using the environment.
        """
        raise NotImplementedError

    def reset(self, seed: typ.Optional[int] = None):
        """Reset needs to initialize the following attributes:
            - agents
            - rewards
            - _cumulative_rewards
            - dones
            - infos
            - agent_selection
        And must set up the environment so that render(), step(), and observe()
        can be called without issues.
        """
        if seed is not None:
            self.seed(seed)

        self.agents = self.possible_agents[:]
        self.rewards = {agent: 0 for agent in self.agents}
        self._cumulative_rewards = {agent: 0 for agent in self.agents}
        self.dones = {agent: False for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}
        self.grass = (
            self.np_random.integers(MAP_MIN, MAP_MAX, 2 * AMOUNT_GRASS)
            .astype(PositionFloat)
            .reshape(2, -1)
        )
        self.state = {
            agent: self.np_random.integers(MAP_MIN, MAP_MAX, 2).astype(
                PositionFloat
            )
            for agent in self.agents
        }
        self.observations = {
            agent: np.concatenate([self.state[agent], self.grass.reshape(-1)])
            for agent in self.agents
        }
        self.num_moves = 0

        # cycle through the agents
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.next()

    def step(self, action: int):
        """Take in an action for the current agent (specified by
        agent_selection) and needs to update:
        - rewards
        - _cumulative_rewards
        - dones
        - infos
        - agent_selection (to the next agent)
        And any internal state used by observe() or render()
        """
        if self.dones[self.agent_selection]:
            """
            handles stepping an agent which is already done
            accepts a None action for the one agent, and moves the
            agent_selection to the next done agent, or if there are no more
            done agents, to the next live agent
            """
            return self._was_done_step(action)

        agent = self.agent_selection
        """
        the agent which stepped last had its _cumulative_rewards accounted for
        (because it was returned by last()), so the _cumulative_rewards for
        this agent should start again at 0
        """
        self._cumulative_rewards[agent] = 0

        move = ACTION_MAP[action]
        # stores action of current agent
        agent_pos = self.state[self.agent_selection]
        agent_pos += move
        agent_pos = np.clip(agent_pos, MAP_MIN, MAP_MAX)
        self.state[self.agent_selection] = agent_pos

        # collect reward if it is the last agent to act
        if self._agent_selector.is_last():
            # rewards for all agents are placed in the .rewards dictionary
            for iagent, agent in enumerate(self.agents):
                agent_pos = self.state[agent]

                def distance(a, b):
                    d = np.linalg.norm(a - b)
                    return d

                reward = min(
                    distance(agent_pos, grass_pos) for grass_pos in self.grass
                )
                self.rewards[agent] = reward

            self.num_moves += 1
            # The dones dictionary must be updated for all players.
            self.dones = {
                agent: self.num_moves >= NUM_ITERS for agent in self.agents
            }

            # observe the current state
            for agent in self.agents:
                iagent = self.agent_name_mapping[agent]
                self.observations[iagent] = self.state[agent]
        else:
            # necessary so that observe() returns a reasonable observation at all times.
            iagent = 1 - self.agent_name_mapping[agent]
            self.state[self.agents[iagent]] = None
            # no rewards are allocated until both players give an action
            self._clear_rewards()

        # selects the next agent.
        self.agent_selection = self._agent_selector.next()
        # Adds .rewards to ._cumulative_rewards
        self._accumulate_rewards()


def env():
    """Add PettingZoo wrappers to environment class."""
    env = RawEnv()
    env = wrappers.AssertOutOfBoundsWrapper(env)
    env = wrappers.OrderEnforcingWrapper(env)
    return env


class RandomWalkAgent:
    def __call__(self, action_space, observation, reward, info):
        return action_space.sample()


EPS = 0.0001


class IterativeWeightOptimizationAgent:
    def __init__(self):
        self.is_initialized = False

    def __call__(self, action_space, observation, reward, info):
        MIN_WEIGHT = 0.05
        learning_rate = 0.01
        learning_randomness = 0.00

        LAST_ACTION_KEY = "last_action"
        LAST_REWARD_KEY = "last_reward"
        ACTIONS_WEIGHTS = "actions_weights"

        if not self.is_initialized:
            info[ACTIONS_WEIGHTS] = np.repeat([1.0], action_space.n)
            self.is_initialized = True

        print("step:", reward, observation)
        last_action = info.get(LAST_ACTION_KEY)
        last_reward = info.get(LAST_REWARD_KEY, 0)
        action_weights = info[ACTIONS_WEIGHTS]
        # avoid big weight change on the first valid step
        if last_action is not None and last_reward > EPS:
            last_action_reward_delta = reward - last_reward
            last_action_weight = action_weights[last_action]
            print(
                "dreward",
                last_action_reward_delta,
                last_action,
                ACTION_MAP[last_action],
            )
            last_action_weight += last_action_reward_delta * learning_rate
            last_action_weight = max(MIN_WEIGHT, last_action_weight)
            action_weights[last_action] = last_action_weight
            print("action_weights", action_weights)

            weight_sum = np.sum(action_weights)
            action_weights /= weight_sum

        def cdf(ds):
            res = {}
            x = 0
            for k, v in ds:
                x += v
                res[k] = x
            for k in res:
                res[k] /= x
            return res

        def choose(cdf):
            assert cdf
            x = random.uniform(0, 1 - EPS)
            k = None
            for k, v in cdf.items():
                if x >= v:
                    return k
            return k

        action_weights_cdf = cdf(enumerate(action_weights))
        print(
            "cdf",
            ", ".join(
                [
                    f"{ACTION_MAP[iaction]}: {w}"
                    for iaction, w in action_weights_cdf.items()
                ]
            ),
        )

        pprint(action_weights_cdf)
        action = choose(action_weights_cdf)
        if random.uniform(0, 1) < learning_randomness:
            action = action_space.sample()
        info[LAST_ACTION_KEY] = action
        info[LAST_REWARD_KEY] = reward
        print("chose action", action, ACTION_MAP[action])
        return action


def main(env: RawEnv):
    policy = IterativeWeightOptimizationAgent()
    env.reset()
    for agent in env.agent_iter():
        observation, reward, done, info = env.last()

        action_space: Discrete = env.action_space(agent)
        if not done:
            action = policy(action_space, observation, reward, info)
            assert action in action_space
        else:
            action = None
        env.step(action)
        env.render("human")
    wait = input("Close?")


if __name__ == "__main__":
    e = env()
    main(e)
