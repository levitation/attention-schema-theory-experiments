import logging
from collections import namedtuple
from typing import Dict, List, NamedTuple, Optional, Tuple

import numpy as np
import numpy.typing as npt
import pygame

import gymnasium.spaces  # cannot import gymnasium.spaces.Tuple directly since it is already used by typing
from aintelope.environments.env_utils.distance import distance_to_closest_item
from aintelope.environments.env_utils.render_ascii import AsciiRenderState
from aintelope.environments.savanna_safetygrid import (
    AGENT_CHR1,
    AGENT_CHR2,
    FOOD_CHR,
    INFO_AGENT_OBSERVATION_COORDINATES,
    INFO_AGENT_OBSERVATION_LAYERS_ORDER,
)
from aintelope.aintelope_typing import (
    Action,
    AgentId,
    AgentStates,
    Info,
    Observation,
    ObservationFloat,
    PositionFloat,
    Reward,
)
from gymnasium.spaces import Box, Discrete
from gymnasium.utils import seeding

logger = logging.getLogger("aintelope.environments.savanna")


Step = Tuple[
    Dict[AgentId, Observation],
    Dict[AgentId, Reward],
    Dict[AgentId, bool],
    Dict[AgentId, bool],
    Dict[AgentId, Info],
]
ACTION_MAP = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]], dtype=PositionFloat)


class RenderSettings:
    def __init__(self, metadata):
        prefix = "render_"
        settings = {
            (k.lstrip(prefix), v) for k, v in metadata.items() if k.startswith(prefix)
        }
        self.__dict__.update(settings)


class RenderState:
    def __init__(self, settings):
        canvas = pygame.Surface((settings.window_size, settings.window_size))
        self.canvas = canvas
        self.settings = settings

    def render(self, agents_state, grass):
        window_size = self.settings.window_size
        canvas = self.canvas

        canvas.fill((255, 255, 255))
        scale = window_size / self.settings.map_max

        screen_m = np.identity(2, dtype=PositionFloat) * scale

        def project(p):
            return np.matmul(p, screen_m).astype(np.int32)

        for gr in grass.reshape((2, -1)):
            p = project(gr)
            pygame.draw.circle(
                canvas,
                self.settings.grass_color,
                p,
                scale * self.settings.grass_radius,
            )

        for agent, agent_pos in agents_state.items():
            assert len(agent_pos) == 2, ("malformed agent_pos", agent_pos)
            # TODO: render agent name as text
            p = project(agent_pos)
            pygame.draw.circle(
                canvas,
                self.settings.agent_color,
                p,
                scale * self.settings.agent_radius,
            )


class HumanRenderState:
    def __init__(self, settings):
        self.fps = settings.fps

        window_size = settings.window_size

        pygame.init()
        pygame.display.init()
        self.window = pygame.display.set_mode((window_size, window_size))
        self.clock = pygame.time.Clock()

    def render(self, render_state):
        # The following line copies our drawings from `canvas` to the visible window
        self.window.blit(render_state.canvas, render_state.canvas.get_rect())
        pygame.event.pump()
        pygame.display.update()

        # We need to ensure that human-rendering occurs at the predefined framerate.
        # The following line will automatically add a delay
        # to keep the framerate stable.
        self.clock.tick(self.fps)


def reward_agent(min_grass_distance):
    # For now measure if agent can eat.
    # Was 1 / (1 + min_grass_distance), moving to instincts
    if min_grass_distance > 1:
        return np.float64(0.0)
    else:
        return np.float64(1.0)


def move_agent(
    agent_pos: np.ndarray, action: Action, map_min=0, map_max=100
) -> np.ndarray:
    move = ACTION_MAP[action]
    agent_pos = agent_pos + move
    agent_pos = np.clip(agent_pos, map_min, map_max)
    return agent_pos


# These methods are temporary, and will not scale for more agents nor more grasses
# they are for instincts, and should be rewritten such that state (observation)
# contains the information necessary for instincts (and distinction for models)
def get_grass_pos_from_state(agent_state, info) -> List[PositionFloat]:
    if len(agent_state.shape) == 3:  # new obseration format
        if (
            False
        ):  # enable if you want to use raw 3D observation and no coordinates in info
            grass_layer_index = info[INFO_AGENT_OBSERVATION_LAYERS_ORDER].index(
                FOOD_CHR
            )
            grass_layer = agent_state[grass_layer_index]
            (row_indices, col_indices) = np.where(grass_layer)
            coordinates = list(zip(row_indices, col_indices))
            return np.array(coordinates)
        else:
            coordinates = info[INFO_AGENT_OBSERVATION_COORDINATES][FOOD_CHR]
            return np.array(coordinates)
    else:
        return [agent_state[2], agent_state[3]]


def get_agent_pos_from_state(agent_state, info, agent_name) -> List[PositionFloat]:
    if len(agent_state.shape) == 3:  # new obseration format
        agent_chr = agent_name[-1]  # TODO: use env.agent_mapping instead
        if (
            False
        ):  # enable if you want to use raw 3D observation and no coordinates in info
            grass_layer_index = info[INFO_AGENT_OBSERVATION_LAYERS_ORDER].index(
                agent_chr
            )
            grass_layer = agent_state[grass_layer_index]
            (row_indices, col_indices) = np.where(grass_layer)
            coordinates = list(zip(row_indices, col_indices))
            return list(coordinates[0])
        else:
            coordinates = info[INFO_AGENT_OBSERVATION_COORDINATES][agent_chr]
            return list(coordinates[0])
    else:
        return [agent_state[0], agent_state[1]]


class SavannaEnv:
    # @zoo-api
    metadata = {
        "render.modes": ["human", "ansi", "rgb_array"],  # needed for zoo
        "name": "savanna-v2",
        "render_agent_radius": 5,
        "render_agent_color": (200, 50, 0),
        "render_grass_radius": 5,
        "render_grass_color": (20, 200, 0),
        "render_modes": ("human", "ascii", "offline"),
        "render_window_size": 512,
        "amount_agents": 1,
        "map_min": 0,
        "map_max": 10,
        "amount_grass_patches": 2,
        "amount_water_holes": 0,
        "num_iters": 1,
        "test_death": False,
        "test_death_probability": 0.33,
        "seed": None,  # used for providing seed via env_params during some tests
    }

    def __init__(self, env_params: Optional[Dict] = None):
        if env_params is None:
            env_params = {}

        # NB! Need to clone in order to not modify the default dict.
        # Similar problem to mutable default arguments.
        self.metadata = dict(self.metadata)
        self.metadata.update(env_params)
        logger.info(f"initializing savanna env with params: {self.metadata}")

        # @zoo-api
        self.possible_agents = [
            f"agent_{r}" for r in range(self.metadata["amount_agents"])
        ]

        # for @zoo-api
        self._action_spaces = {
            agent: Discrete(4) for agent in self.possible_agents
        }  # agents can walk in 4 directions

        # for @zoo-api
        self._observation_spaces = {
            agent: gymnasium.spaces.Tuple(
                [
                    Box(
                        low=self.metadata["map_min"],
                        high=self.metadata["map_max"],
                        shape=(
                            2
                            * (
                                self.metadata["amount_agents"]
                                + self.metadata["amount_grass_patches"]
                                + self.metadata["amount_water_holes"]
                            ),
                        ),
                    ),
                    Box(
                        low=-np.inf, high=np.inf, shape=(2,)
                    ),  # dummy interoception vector
                ]
            )
            for agent in self.possible_agents
        }
        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(self.metadata["amount_agents"])))
        )

        # our own state
        self.agent_states: AgentStates = {}
        self.seed(self.metadata["seed"])

        render_settings = RenderSettings(self.metadata)
        self.render_state = RenderState(render_settings)
        self.human_render_state = None
        self.ascii_render_state = None
        self.dones = None
        self.infos = {
            agent: {} for agent in self.possible_agents
        }  # needed for Zoo sequential API
        self.dummy_interoception_vector = np.zeros([2], np.float32)

    def seed(self, seed: Optional[int] = None) -> None:
        self.np_random, seed = seeding.np_random(seed)

    def reset(self, seed: Optional[int] = None, options=None):
        """Reset needs to initialize the following attributes:
            - agents
            - rewards
            - _cumulative_rewards
            - dones
            - infos
            - agent_selection
        And must set up the environment so that render(), step(), and observe()
        can be called without issues.
        """
        # if seed is not provided as an argument to reset() then do not re-seed.
        # It is possible that seed was set during construction
        if seed is not None:
            self.seed(seed)

        self.agents = list(
            self.possible_agents
        )  # clone since the agents list may be modified when some agent dies
        self.rewards = {
            agent: 0.0 for agent in self.agents
        }  # storing in self is needed for Zoo sequential API
        self._cumulative_rewards = {agent: 0.0 for agent in self.agents}
        self.grass_patches = self.np_random.integers(
            self.metadata["map_min"],
            self.metadata["map_max"],
            size=(self.metadata["amount_grass_patches"], 2),
        ).astype(PositionFloat)
        self.water_holes = self.np_random.integers(
            self.metadata["map_min"],
            self.metadata["map_max"],
            size=(self.metadata["amount_water_holes"], 2),
        ).astype(PositionFloat)
        self.agent_states = {
            agent: self.np_random.integers(
                self.metadata["map_min"], self.metadata["map_max"], 2
            ).astype(PositionFloat)
            for agent in self.agents
        }
        self.num_moves = 0

        # # cycle through the agents; needed for wrapper
        self.dones = {agent: False for agent in self.agents}
        observations = {agent: self.observe(agent) for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}
        return observations, self.infos

    def step(self, actions: Dict[str, Action]) -> Step:
        """step(action) takes in an action for each agent and should return the
        - observations
        - rewards
        - dones
        - truncateds
        - info
        dicts where each dict looks like:
            {agent_1: action_of_agent_1, agent_2: action_of_agent_2}
        or generally:
            {<agent_name>: <agent_action or None if agent is done>}
        """
        logger.debug("debug actions", actions)
        # If a user passes in actions with no agents,
        # then just return empty observations, etc.
        if not actions:
            return {}, {}, {}, {}, {}

        if self.agents == []:
            raise ValueError("No agents found; num_iters reached?")

        self.rewards = {}  # storing in self is needed for Zoo sequential API
        for agent in self.agents:
            action = actions.get(agent)
            if isinstance(action, dict):
                action = action.get(agent)

            if action is not None:
                if self.dones[agent]:  # non-None action on done agent?
                    raise ValueError(
                        "When an agent is dead, the only valid action is None"
                    )

                logger.debug("debug action", action)
                self.agent_states[agent] = move_agent(
                    self.agent_states[agent],
                    action,
                    map_min=self.metadata["map_min"],
                    map_max=self.metadata["map_max"],
                )

            # NB! reward should be calculated for all agents,
            # including those who were not specified in actions
            min_grass_distance = distance_to_closest_item(
                self.agent_states[agent], self.grass_patches
            )
            self.rewards[agent] = reward_agent(min_grass_distance)
            self._cumulative_rewards[agent] += self.rewards[agent]

            # NB! any agent could die at any other agent's step
            if (
                self.metadata["test_death"]
                and self.np_random.random() < self.metadata["test_death_probability"]
            ):
                self.dones[agent] = True

        self.num_moves += 1

        if self.num_moves >= self.metadata["num_iters"]:
            self.dones = {agent: True for agent in self.agents}

        observations = {agent: self.observe(agent) for agent in self.agents}

        # typically there won't be any information in the infos, but there must
        # still be an entry for each agent
        infos: Dict[AgentId, dict] = {agent: {} for agent in self.agents}

        logger.debug("debug return", observations, self.rewards, self.dones, infos)

        truncateds = {key: False for key in self.dones.keys()}
        return (
            observations,
            self.rewards,
            self.dones,
            truncateds,
            infos,
        )

    def observe(self, agent: str) -> npt.NDArray[ObservationFloat]:
        """Return observation of given agent."""

        def stack(*args) -> npt.NDArray[ObservationFloat]:
            return np.hstack(args, dtype=ObservationFloat)

        observations = np.concatenate(
            [self.agent_states[agent2] for agent2 in self.possible_agents]
        )
        for x in self.grass_patches:
            observations = stack(observations, x)
        for x in self.water_holes:
            observations = stack(observations, x)
        # just put all positions into one row
        res = observations.reshape(-1)
        assert (
            res.shape == next(iter(self._observation_spaces.values()))[0].shape
        ), "observation / observation space shape mismatch"
        return (res, self.dummy_interoception_vector)

    def set_agent_position(self, agent: str, loc: npt.NDArray[ObservationFloat]):
        """Move the agent to a location. Tests and inference"""
        self.agent_states[agent] = loc

    def observe_from_location(self, agents_coordinates: Dict):
        """This method is read-only (does not change the actual state of the
        environment nor the actual state of agents). Each given agent observes itself
        and the environment as if the agent was in the given location.
        """
        observations = {}
        for agent, coordinate in agents_coordinates.items():
            original_coordinate = self.agent_states[agent]  # save original state
            self.set_agent_position(agent, np.array(coordinate))
            observations[agent] = self.observe(agent)
            self.set_agent_position(
                agent, original_coordinate
            )  # restore original state
        return observations

    def state_to_namedtuple(self, state: npt.NDArray[ObservationFloat]) -> NamedTuple:
        """Method to convert a state array into a named tuple."""
        if state is None:
            return None
        agent_coords = {"agent_coords": state[:2]}
        grass_patches_coords = {}
        gp_offset = 2
        water_holes_coords = {}
        wh_offset = 2 + self.metadata["amount_grass_patches"] * 2
        for i in range(self.metadata["amount_grass_patches"]):
            grass_patches_coords[f"grass_patch_{i}"] = state[
                gp_offset + i : gp_offset + i + 2
            ]
        for i in range(self.metadata["amount_water_holes"]):
            water_holes_coords[f"water_hole_{i}"] = state[
                wh_offset + i : wh_offset + i + 2
            ]

        keys = (
            list(agent_coords) + list(grass_patches_coords) + list(water_holes_coords)
        )
        StateTuple = namedtuple("StateTuple", {k: np.ndarray for k in keys})
        return StateTuple(**agent_coords, **grass_patches_coords, **water_holes_coords)

    def render(self, mode="human"):
        """Render the environment."""

        self.render_state.render(self.agent_states, self.grass_patches)

        if mode == "human":
            if not self.human_render_state:
                self.human_render_state = HumanRenderState(self.render_state.settings)
            self.human_render_state.render(self.render_state)
        elif mode == "ascii":
            if not self.ascii_render_state:
                self.ascii_render_state = AsciiRenderState(
                    self.agent_states,
                    self.grass_patches,
                    self.render_state.settings,
                )
            self.ascii_render_state.render(self.agent_states, self.grass_patches)
        else:  # rgb_array
            return np.transpose(
                np.array(pygame.surfarray.pixels3d(self.render_state.canvas)),
                axes=(1, 0, 2),
            )

    def close(self):
        """Release any graphical display, subprocesses, network connections
        or any other environment data which should not be kept around after
        the user is no longer using the environment.
        """
        pass
