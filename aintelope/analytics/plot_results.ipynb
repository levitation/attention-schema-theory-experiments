{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for Aintelope\n",
    "Run these blocks for all tests, then scroll to the title you're interested in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import dateutil.parser as dparser\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from aintelope.training.dqn_training import load_checkpoint\n",
    "from aintelope.training.dqn_training import Trainer\n",
    "from aintelope.training.dqn_training import ReplayMemory\n",
    "\n",
    "import aintelope.agents\n",
    "from aintelope.agents import get_agent_class\n",
    "from aintelope.agents import q_agent, instinct_agent\n",
    "\n",
    "from aintelope.environments.savanna_gym import SavannaGymEnv\n",
    "from aintelope.environments.savanna_zoo import SavannaZooSequentialEnv\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import hydra\n",
    "from hydra.core import global_hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and directories\n",
    "outputs_dir = root_dir + \"/outputs/\"\n",
    "available_records = os.listdir(outputs_dir)\n",
    "print(\"Existing training runs\", available_records)\n",
    "conf_dir = root_dir + \"/aintelope/config/config_experiment.yaml\"\n",
    "\n",
    "available_records = os.listdir(outputs_dir)\n",
    "dirs = [\n",
    "    os.path.join(outputs_dir, f) for f in available_records\n",
    "]  # add path to each file\n",
    "dirs.sort(key=lambda x: os.path.getmtime(x))\n",
    "assert len(dirs) > 0, \"No trainings have been run! make run-training* first\"\n",
    "\n",
    "# sort by experiment type (baseline/instinct)\n",
    "global_hydra.GlobalHydra.instance().clear()\n",
    "with initialize(version_base=None, config_path=\"../config\"):\n",
    "    # conf_dir = root_dir+'/aintelope/config/config_experiment.yaml'\n",
    "    conf_dir = \"config_experiment.yaml\"\n",
    "    cfg = compose(config_name=conf_dir, overrides=[])  # OmegaConf.load(conf_dir)\n",
    "    cfg_base = compose(\n",
    "        config_name=conf_dir,\n",
    "        overrides=[\n",
    "            \"hparams.agent_id=q_agent\",\n",
    "            \"hparams.agent_params.target_instincts=[]\",\n",
    "        ],\n",
    "    )\n",
    "    cfg_inst = compose(\n",
    "        config_name=conf_dir,\n",
    "        overrides=[\n",
    "            \"hparams.agent_id=instinct_agent\",\n",
    "            \"hparams.agent_params.target_instincts=['smell']\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "models = {}\n",
    "for exp_dir in dirs:\n",
    "    dirr = \"../../outputs/\" + exp_dir.split(\"/\")[-1] + \"/hydra_logs/.hydra\"\n",
    "    # print(exp_dir)\n",
    "    with initialize(version_base=None, config_path=dirr):  # \"../config\"\n",
    "        conf_dir = \"config.yaml\"\n",
    "        cfg = compose(config_name=conf_dir, overrides=[])\n",
    "    mode = cfg.hparams.agent_id\n",
    "    # print(mode)\n",
    "    mod_dir = os.listdir(exp_dir + \"/checkpoints/\")\n",
    "    runs_dir = [os.path.join(exp_dir + \"/checkpoints/\", m) for m in mod_dir]\n",
    "    runs_dir.sort(key=lambda x: os.path.getmtime(x))\n",
    "    if mode not in models:\n",
    "        models[mode] = []\n",
    "    models[mode].append(runs_dir)\n",
    "# print(models)\n",
    "latest_exp_dir = dirs[-1]  # arbitrary use of latest\n",
    "\n",
    "print(available_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old plots\n",
    "There might be an issue with env resets? The plots seem to have some jumps and multitude of grasspatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latest_exp_dir)\n",
    "print(dparser.parse(latest_exp_dir, fuzzy=True))\n",
    "df = pd.read_csv(latest_exp_dir + \"/memory_records.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "keys = (\n",
    "    [\"agent_coords\"]\n",
    "    + [f\"grass_patch_{i}\" for i in range(cfg.hparams.env_params.amount_grass_patches)]\n",
    "    + [f\"water_hole_{i}\" for i in range(cfg.hparams.env_params.amount_water_holes)]\n",
    ")\n",
    "StateTuple = namedtuple(\"StateTuple\", {k: np.ndarray for k in keys})\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "event_x = []\n",
    "event_y = []\n",
    "event_type = []\n",
    "food_x = []\n",
    "food_y = []\n",
    "water_x = []\n",
    "water_y = []\n",
    "for _, row in df.iterrows():\n",
    "    state = eval(row[\"state\"])\n",
    "    # print(state)\n",
    "    x.append(state[0][0])\n",
    "    y.append(state[0][1])\n",
    "\n",
    "    # refactor\n",
    "    food_x.append(state[1][0])\n",
    "    food_y.append(state[1][1])\n",
    "    # food_x.append(state[2][0])\n",
    "    # food_y.append(state[2][1])\n",
    "\n",
    "    if row[\"instinct_events\"] != \"[]\":\n",
    "        event_x.append(x[-1])\n",
    "        event_y.append(y[-1])\n",
    "        event_type.append(row[\"instinct_events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(data={\"x\": x, \"y\": y})\n",
    "print(agent_df.head(), len(agent_df))\n",
    "\n",
    "food_df = pd.DataFrame(data={\"x\": food_x, \"y\": food_y})\n",
    "print(food_df.head(), len(food_df))\n",
    "\n",
    "# water_df = pd.DataFrame(data={'x':water_x, 'y':water_y})\n",
    "# print(water_df.head(), len(water_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.DataFrame(data={\"x\": event_x, \"y\": event_y, \"event_type\": event_type})\n",
    "print(len(event_df))\n",
    "print(event_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_df[\"x\"], agent_df[\"y\"], \".r-\")\n",
    "plt.plot(food_df[\"x\"], food_df[\"y\"], \".g\", markersize=15)\n",
    "# plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward received over time\n",
    "df[\"reward\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = \"autumn\"  # starts yellow, goes orange, then red\n",
    "n_points = len(agent_df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "cm = plt.get_cmap(color_map)\n",
    "for i in range(10):\n",
    "    ax1.set_prop_cycle(\"color\", cm(np.linspace(0, 1, n_points - 1, endpoint=False)))\n",
    "    for i in range(n_points - 1):\n",
    "        plt.plot(agent_df[\"x\"][i : i + 2], agent_df[\"y\"][i : i + 2])\n",
    "plt.plot(food_df[\"x\"], food_df[\"y\"], \".g\", markersize=15)\n",
    "# plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuemaps for actions\n",
    "Note, these maps are new ones and don't correlate with the above ones as we randomly regenerate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space\n",
    "observation, info = env.reset()\n",
    "n_obs = observation.shape\n",
    "model = load_checkpoint(\n",
    "    models[\"q_agent\"][-1][-1], n_obs, action_space.n\n",
    ")  # latest with last model done.\n",
    "trainer = Trainer(cfg)\n",
    "agent_id = \"agent_0\"\n",
    "agent = get_agent_class(cfg.hparams.agent_id)(\n",
    "    agent_id,\n",
    "    trainer,\n",
    "    **cfg.hparams.agent_params,\n",
    ")\n",
    "# move the agent into each square and ask for its values for each action, then add that direction into the map\n",
    "valuemap = np.zeros((env.metadata[\"map_max\"] + 2, env.metadata[\"map_max\"] + 2, 4))\n",
    "\n",
    "ACTION_MAP = np.array(\n",
    "    [[0, 1], [1, 0], [0, -1], [-1, 0]]\n",
    ")  # This is a copy from savanna.py, should be an accessible param\n",
    "for x in range(0, env.metadata[\"map_max\"]):\n",
    "    for y in range(0, env.metadata[\"map_max\"]):\n",
    "        if (env.grass_patches == [x, y]).all(1).any():\n",
    "            continue\n",
    "        if (env.water_holes == [x, y]).all(1).any():\n",
    "            continue\n",
    "        env.set_agent_position(agent, np.array([x, y]))\n",
    "        observation = env.observe(agent)\n",
    "        # print(env.agent_states[agent])\n",
    "        action_vals = model(Tensor(observation)).detach().numpy()\n",
    "        offset = ACTION_MAP\n",
    "        for action in range(len(ACTION_MAP)):\n",
    "            x_ = offset[action][0] + x\n",
    "            y_ = offset[action][1] + y\n",
    "            valuemap[x_, y_, action] = action_vals[action]\n",
    "\n",
    "valuemap = np.sum(valuemap, 2) / len(ACTION_MAP)\n",
    "\n",
    "# print(valuemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(valuemap[1:-1, 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render() isnt working atm\n",
    "maps = np.zeros((env.metadata[\"map_max\"], env.metadata[\"map_max\"]))\n",
    "for grs in env.grass_patches:\n",
    "    print(grs[0])\n",
    "    maps[int(grs[0]), int(grs[1])] = 1.0\n",
    "for wtr in env.water_holes:\n",
    "    maps[int(wtr[0]), int(wtr[1])] = 2.0\n",
    "sns.heatmap(maps)\n",
    "# RED FOOD, LIGHT water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agent_position(agent, np.array([2, 2]))\n",
    "observation = env.observe(agent)\n",
    "action_vals = model(Tensor(observation)).detach().numpy()\n",
    "print(action_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance plots\n",
    "These plots don't have the exploration bonus as confabulators (such as epsilon-greedy).\n",
    "\n",
    "Train the model N times, then change n_latest to this N and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testrun(model, env, test_cfg, samples, test_length):\n",
    "    # Run each model for 10 different resets, with 10 different locations on the map.\n",
    "    # Gather the cumulative reward, -1 has to be given on each step though\n",
    "    # Or reset each time the food is found?\n",
    "\n",
    "    env.reset()\n",
    "    trainer = Trainer(cfg)\n",
    "    agent_id = \"agent_0\"\n",
    "    observation = env.observe(agent_id)\n",
    "    agent = get_agent_class(cfg.hparams.agent_id)(\n",
    "        agent_id,\n",
    "        trainer,\n",
    "        **cfg.hparams.agent_params,\n",
    "    )\n",
    "    trainer.add_agent(agent_id, observation.shape, env.action_space)\n",
    "    coords = np.arange(env.metadata[\"map_min\"], env.metadata[\"map_max\"], 1)\n",
    "    start_pos = np.random.choice(coords, size=(samples, 2))\n",
    "    scores = []\n",
    "    rewards = []\n",
    "    for j in range(start_pos.shape[0]):\n",
    "        env.reset()\n",
    "        agent.reset(env.observe(agent.id))\n",
    "        env.set_agent_position(agent, np.array(start_pos[j]))\n",
    "        for i in range(test_length):\n",
    "            observation = env.observe(agent.id)\n",
    "            action = agent.get_action(observation, 0)\n",
    "\n",
    "            observation, score, terminateds, truncateds, _ = env.step(action)\n",
    "            reward = agent.update(env, observation, score, False)\n",
    "            rewards.append(reward)\n",
    "            scores.append(score)\n",
    "\n",
    "    return scores, rewards, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from checkpoints and run them through test envs to measure their performance\n",
    "# without training parameters (usually exploration, like epsilon). statistical significance per run\n",
    "def calc_results_per_model(models, test_cfg):  # test_dirs\n",
    "    samples = 10  # how many times a random start location is attempted\n",
    "    test_length = 20  # how long time to find grass\n",
    "\n",
    "    env = SavannaZooSequentialEnv(env_params=test_cfg.hparams.env_params)\n",
    "    env.reset()\n",
    "    observation = env.observe(\"agent_0\")\n",
    "    n_actions = env.action_space(\"agent_0\").n\n",
    "    n_obs = observation.shape\n",
    "    scores = np.zeros([len(models[0]), len(models), samples * test_length])\n",
    "    rewards = np.zeros([len(models[0]), len(models), samples * test_length])\n",
    "    for i in range(len(models)):  # different runs for statistical significance\n",
    "        for j in range(len(models[0])):  # model as epochs progress\n",
    "            model = load_checkpoint(models[i][j], n_obs, n_actions)\n",
    "            model.eval()\n",
    "            # run model\n",
    "            score, reward, agent = testrun(model, env, test_cfg, samples, test_length)\n",
    "            scores[j, i, :] = score\n",
    "            rewards[j, i, :] = reward\n",
    "    return scores, rewards, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# Style | epoch x 4-corners x repetition\n",
    "# Two np arrays with [10,4,10] dimensions\n",
    "# Style=baseline or instinct, 4-corners is in above the different starting positions\n",
    "# (i.e. statistical stability) and repetitions is for all the folders ya made, (stat stab)\n",
    "\n",
    "# Split directories here for which ones in outputs are from instinct and which are baseline\n",
    "base_dirs = models[\"q_agent\"]\n",
    "inst_dirs = models[\"instinct_agent\"]\n",
    "\n",
    "b_scores, b_rewards, b_agent = calc_results_per_model(base_dirs, cfg_base)\n",
    "i_scores, i_rewards, i_agent = calc_results_per_model(inst_dirs, cfg_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\"Reshapes numpy array a by combining count dimensions,\n",
    "    starting at dimension index start\"\"\"\n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start + count :])\n",
    "\n",
    "\n",
    "print(b_scores.shape, b_rewards.shape)\n",
    "print(i_scores.shape, i_rewards.shape)\n",
    "print(b_agent, i_agent)\n",
    "print(b_rewards.shape)\n",
    "\"\"\" As discrete datapoints, epoch x model_run x (starts + test_length)\n",
    "b_s = combine_dims(b_scores[0:-1,:,:],1).T\n",
    "b_r = combine_dims(b_rewards[0:-1,:,:],1).T\n",
    "i_s = combine_dims(i_scores[0:-1,:,:],1).T\n",
    "i_r = combine_dims(i_rewards[0:-1,:,:],1).T\n",
    "\"\"\"\n",
    "b_s = np.mean(b_scores, axis=2).T\n",
    "b_r = np.mean(b_rewards, axis=2).T\n",
    "i_s = np.mean(i_scores, axis=2).T\n",
    "i_r = np.mean(i_rewards, axis=2).T\n",
    "# print(b_r, i_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "X = np.arange(1, b_scores.shape[0] + 1)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey=True)\n",
    "axs[0].boxplot(b_r, labels=X)\n",
    "axs[0].set_title(\"Baseline\")\n",
    "\n",
    "axs[1].boxplot(i_r, labels=X)\n",
    "axs[1].set_title(\"Instinct\")\n",
    "\n",
    "# plt.boxplot(i_r, labels=X)\n",
    "# plt.boxplot(i_r, labels=X)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "# plt.title(\"Reward comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score results\n",
    "\n",
    "b_mean = b_s.flatten()\n",
    "i_mean = i_s.flatten()\n",
    "if b_s.shape[0] > 1:\n",
    "    b_mean = np.mean(b_s, axis=0)\n",
    "if i_s.shape[0] > 1:\n",
    "    i_mean = np.mean(i_s, axis=0)\n",
    "\n",
    "X = np.arange(1, b_scores.shape[0] + 1)\n",
    "\n",
    "plt.plot(X, b_mean, color=\"r\", label=\"baseline\")\n",
    "plt.plot(X, i_mean, color=\"g\", label=\"instinct\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Score comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add statsign boxes\n",
    "# figure out what the rewards should be from agent\n",
    "# figure out what the score should be\n",
    "# do 10 epochs\n",
    "# https://matplotlib.org/3.1.1/gallery/statistics/boxplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
