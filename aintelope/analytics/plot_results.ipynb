{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for Aintelope\n",
    "Run these blocks for all tests, then scroll to the title you're interested in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import dateutil.parser as dparser\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from aintelope.training.lightning_trainer import DQNLightning\n",
    "from aintelope.agents.memory import ReplayBuffer\n",
    "import aintelope.agents\n",
    "from aintelope.agents import get_agent_class\n",
    "#from aintelope.agents.inference_agent import InferenceAgent\n",
    "from aintelope.environments.savanna_gym import SavannaGymEnv\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import hydra\n",
    "from hydra.core import global_hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = root_dir+'/outputs/' \n",
    "available_records = os.listdir(outputs_dir)\n",
    "print(\"Existing training runs\", available_records)\n",
    "\n",
    "dirs = [os.path.join(outputs_dir, f) for f in available_records] # add path to each file\n",
    "dirs.sort(key=lambda x: os.path.getmtime(x))\n",
    "assert len(dirs) > 0, \"No trainings have been run! make run-training* first\"\n",
    "\n",
    "# sort by experiment type (baseline/instinct) \n",
    "global_hydra.GlobalHydra.instance().clear()\n",
    "with initialize(version_base=None, config_path=\"../config\"):\n",
    "    #conf_dir = root_dir+'/aintelope/config/config_experiment.yaml'\n",
    "    conf_dir = 'config_experiment.yaml'\n",
    "    cfg = compose(config_name=conf_dir, overrides=[])  #OmegaConf.load(conf_dir) \n",
    "    cfg_base = compose(config_name=conf_dir, overrides=[\"hparams.agent_id=q_agent\",\"hparams.agent_params.target_instincts=[]\"])\n",
    "    cfg_inst = compose(config_name=conf_dir, overrides=[\"hparams.agent_id=instinct_agent\",\"hparams.agent_params.target_instincts=['smell']\"])\n",
    "\n",
    "models = {}\n",
    "for exp_dir in dirs:\n",
    "    dirr = \"../../outputs/\"+exp_dir.split('/')[-1]+\"/hydra_logs/.hydra\"\n",
    "    #print(exp_dir)\n",
    "    with initialize(version_base=None, config_path=dirr): #\"../config\"\n",
    "        conf_dir = 'config.yaml'\n",
    "        cfg = compose(config_name=conf_dir, overrides=[])\n",
    "    mode = cfg.hparams.agent_id\n",
    "    #print(mode)\n",
    "    mod_dir = os.listdir(exp_dir+\"/checkpoints/\")\n",
    "    runs_dir = [os.path.join(exp_dir+\"/checkpoints/\", m) for m in mod_dir]\n",
    "    runs_dir.sort(key=lambda x: os.path.getmtime(x))\n",
    "    if mode not in models:\n",
    "        models[mode] = []\n",
    "    models[mode].append(runs_dir)\n",
    "#print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_exp_dir = dirs[0]\n",
    "print(latest_exp_dir)\n",
    "print(dparser.parse(latest_exp_dir,fuzzy=True))\n",
    "df = pd.read_csv(latest_exp_dir+\"/memory_records.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "#cfg = OmegaConf.load(conf_dir_base)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = DQNLightning.load_from_checkpoint(latest_exp_dir+\"/checkpoints/last.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "keys = ([\"agent_coords\"] + \n",
    "        [f\"grass_patch_{i}\" for i in range(env.metadata[\"amount_grass_patches\"])] + \n",
    "        [f\"water_hole_{i}\" for i in range(env.metadata[\"amount_water_holes\"])])\n",
    "StateTuple = namedtuple(\"StateTuple\", {k: np.ndarray for k in keys})\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "event_x = []\n",
    "event_y = []\n",
    "event_type = []\n",
    "food_x = []\n",
    "food_y = []\n",
    "water_x = []\n",
    "water_y = []\n",
    "for _ ,row in df.iterrows():\n",
    "    \n",
    "    state = eval(row['state'])\n",
    "    #print(state)\n",
    "    x.append(state[0][0])\n",
    "    y.append(state[0][1])\n",
    "    \n",
    "    #refactor\n",
    "    food_x.append(state[1][0])\n",
    "    food_y.append(state[1][1])\n",
    "    #food_x.append(state[2][0])\n",
    "    #food_y.append(state[2][1])    \n",
    "\n",
    "    if row['instinct_events'] != '[]':\n",
    "        event_x.append(x[-1])\n",
    "        event_y.append(y[-1])\n",
    "        event_type.append(row['instinct_events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(data={'x':x, 'y':y})\n",
    "print(agent_df.head(), len(agent_df))\n",
    "\n",
    "food_df = pd.DataFrame(data={'x':food_x, 'y':food_y})\n",
    "print(food_df.head(), len(food_df))\n",
    "\n",
    "#water_df = pd.DataFrame(data={'x':water_x, 'y':water_y})\n",
    "#print(water_df.head(), len(water_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.DataFrame(data={'x':event_x, 'y':event_y, 'event_type':event_type})\n",
    "print(len(event_df))\n",
    "print(event_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_df['x'], agent_df['y'], '.r-')\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward received over time\n",
    "df['reward'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = 'autumn' # starts yellow, goes orange, then red\n",
    "n_points = len(agent_df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111) \n",
    "cm = plt.get_cmap(color_map)\n",
    "for i in range(10):\n",
    "    ax1.set_prop_cycle('color', cm(np.linspace(0, 1, n_points - 1, endpoint=False)))\n",
    "    for i in range(n_points - 1):\n",
    "        plt.plot(agent_df['x'][i:i+2], agent_df['y'][i:i+2])\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuemaps for actions\n",
    "Note, these maps are new ones and don't correlate with the above ones as we randomly regenerate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "#cfg = OmegaConf.load(conf_dir)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = DQNLightning.load_from_checkpoint(latest_exp_dir+\"/checkpoints/last.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the agent into each square and ask for its values for each action, then add that direction into the map\n",
    "valuemap = np.zeros((env.metadata['map_max']+2,env.metadata['map_max']+2,4))\n",
    "agent = env.agents[0]\n",
    "\n",
    "ACTION_MAP = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]]) # This is a copy from savanna.py, should be an accessible param\n",
    "for x in range(0, env.metadata['map_max']):\n",
    "    for y in range(0, env.metadata['map_max']):\n",
    "        if (env.grass_patches == [x,y]).all(1).any():\n",
    "            continue\n",
    "        if (env.water_holes == [x,y]).all(1).any():\n",
    "            continue\n",
    "        env.set_agent_position(agent, np.array([x,y]))\n",
    "        observation = env.observe(agent)\n",
    "        #print(env.agent_states[agent])\n",
    "        action_vals = model(Tensor(observation)).detach().numpy()\n",
    "        offset = ACTION_MAP\n",
    "        for action in range(len(ACTION_MAP)):\n",
    "            x_ = offset[action][0]+x\n",
    "            y_ = offset[action][1]+y\n",
    "            valuemap[x_,y_,action] = action_vals[action]\n",
    "            \n",
    "valuemap = np.sum(valuemap,2)/len(ACTION_MAP)\n",
    "\n",
    "#print(valuemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(valuemap[1:-1,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render() isnt working atm\n",
    "maps = np.zeros((env.metadata['map_max'],env.metadata['map_max']))\n",
    "for grs in env.grass_patches:\n",
    "    print(grs[0])\n",
    "    maps[int(grs[0]),int(grs[1])] = 2.0\n",
    "for wtr in env.water_holes:\n",
    "    maps[int(wtr[0]),int(wtr[1])] = 4.0\n",
    "sns.heatmap(maps)\n",
    "# RED FOOD, LIGHT water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agent_position(agent, np.array([2,2]))\n",
    "observation = env.observe(agent)\n",
    "action_vals = model(Tensor(observation)).detach().numpy()\n",
    "print(action_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance plots\n",
    "These plots don't have the exploration bonus as confabulators (such as epsilon-greedy).\n",
    "\n",
    "Train the model N times, then change n_latest to this N and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each model for 10 different resets, with 10 different locations on the map.\n",
    "# Gather the cumulative reward, -1 has to be given on each step though\n",
    "# reset each time the food is found\n",
    "def testrun(model,env,cfg, samples, test_length):\n",
    "    agent = get_agent_class(cfg.hparams.agent_id)(\n",
    "        env,\n",
    "        ReplayBuffer(cfg.hparams.replay_size),\n",
    "        0,\n",
    "        **cfg.hparams.agent_params, #paskaa ei ole list\n",
    "    )\n",
    "    epsilon = 0.0\n",
    "    device = \"cpu\"\n",
    "    coords = np.arange(env.metadata['map_min'], env.metadata['map_max'], 1)\n",
    "    start_pos = np.random.choice(coords, size=(samples,2))\n",
    "    scores = []\n",
    "    rewards = []\n",
    "    for j in range(start_pos.shape[0]):\n",
    "        agent.reset() # !!!\n",
    "        env.set_agent_position(agent, np.array(start_pos[j]))\n",
    "        for i in range(test_length):\n",
    "            reward, score, done = agent.play_step(model, epsilon, device) # !!!\n",
    "            rewards.append(reward) \n",
    "            scores.append(score)\n",
    "            \n",
    "    return scores, rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load the models from checkpoints and run them through test envs to measure their performance \n",
    "# without training parameters (usually exploration, like epsilon). statistical significance per run\n",
    "def calc_results_per_model(models, test_cfg): #test_dirs\n",
    "    '''\n",
    "    models = []\n",
    "    for exp_dir in test_dirs:\n",
    "        mod_dir = os.listdir(exp_dir+\"/checkpoints/\")\n",
    "        runs_dir = [os.path.join(exp_dir+\"/checkpoints/\", m) for m in mod_dir]\n",
    "        runs_dir.sort(key=lambda x: os.path.getmtime(x))\n",
    "        models.append(runs_dir)\n",
    "    '''    \n",
    "    samples = 10 # how many times a random start location is attempted\n",
    "    test_length = 20 # how long time to find grass\n",
    "    \n",
    "    env = SavannaGymEnv(env_params=test_cfg.hparams.env_params)\n",
    "    scores = np.zeros([len(models[0]),len(models),samples*test_length]) \n",
    "    rewards = np.zeros([len(models[0]),len(models),samples*test_length])\n",
    "    for i in range(len(models)): # different runs for statistical significance\n",
    "        for j in range(len(models[0])): # model as epochs progress\n",
    "            model = DQNLightning.load_from_checkpoint(models[i][j]).net\n",
    "            model.eval()\n",
    "            # run model\n",
    "            score, reward, agent = testrun(model, env, test_cfg, samples, test_length)\n",
    "            scores[j,i,:] = score\n",
    "            rewards[j,i,:] = reward\n",
    "    return scores, rewards, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# Style | epoch x 4-corners x repetition\n",
    "# Two np arrays with [10,4,10] dimensions \n",
    "# Style=baseline or instinct, 4-corners is in above the different starting positions \n",
    "# (i.e. statistical stability) and repetitions is for all the folders ya made, (stat stab)\n",
    "\n",
    "# Split directories here for which ones in outputs are from instinct and which are baseline\n",
    "base_dirs = models['q_agent']\n",
    "inst_dirs = models['instinct_agent']\n",
    "\n",
    "b_scores, b_rewards, b_agent = calc_results_per_model(base_dirs, cfg_base)\n",
    "i_scores, i_rewards, i_agent = calc_results_per_model(inst_dirs, cfg_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from aintelope.agents.instincts.savanna_instincts import available_instincts_dict\n",
    "print(available_instincts_dict)\n",
    "for instinct_name in i_agent.target_instincts:\n",
    "    print(instinct_name)\n",
    "    if instinct_name not in available_instincts_dict:\n",
    "        print(\"haa\")\n",
    "print(i_agent.target_instincts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\" Reshapes numpy array a by combining count dimensions, \n",
    "        starting at dimension index start \"\"\"\n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start+count:])\n",
    "\n",
    "print(b_scores.shape, b_rewards.shape)\n",
    "print(i_scores.shape, i_rewards.shape)\n",
    "print(b_agent,i_agent)\n",
    "print(b_rewards.shape)\n",
    "''' As discrete datapoints, epoch x model_run x (starts + test_length)\n",
    "b_s = combine_dims(b_scores[0:-1,:,:],1).T\n",
    "b_r = combine_dims(b_rewards[0:-1,:,:],1).T\n",
    "i_s = combine_dims(i_scores[0:-1,:,:],1).T\n",
    "i_r = combine_dims(i_rewards[0:-1,:,:],1).T\n",
    "'''\n",
    "b_s = np.mean(b_scores[0:-1,:,:],axis=2).T\n",
    "b_r = np.mean(b_rewards[0:-1,:,:],axis=2).T\n",
    "i_s = np.mean(i_scores[0:-1,:,:],axis=2).T\n",
    "i_r = np.mean(i_rewards[0:-1,:,:],axis=2).T\n",
    "#print(b_r, i_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "X = np.arange(1, b_scores[:-1,].shape[0]+1) \n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey=True)\n",
    "axs[0].boxplot(b_r, labels=X)\n",
    "axs[0].set_title('Baseline')\n",
    "\n",
    "axs[1].boxplot(i_r, labels=X)\n",
    "axs[1].set_title('Instinct')\n",
    "\n",
    "#plt.boxplot(i_r, labels=X)\n",
    "#plt.boxplot(i_r, labels=X)\n",
    "\n",
    "plt.xlabel(\"Epoch\") \n",
    "plt.ylabel(\"Reward\") \n",
    "#plt.title(\"Reward comparison\") \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score results\n",
    "X = np.arange(1, b_scores[:-1,].shape[0]+1) \n",
    "plt.plot(X, np.mean(b_s,axis=1), color='r', label='baseline') \n",
    "plt.plot(X, np.mean(i_s,axis=1), color='g', label='instinct') \n",
    "\n",
    "plt.xlabel(\"Epoch\") \n",
    "plt.ylabel(\"Score\") \n",
    "plt.title(\"Score comparison\") \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add statsign boxes\n",
    "# figure out what the rewards should be from agent\n",
    "# figure out what the score should be\n",
    "# do 10 epochs\n",
    "# https://matplotlib.org/3.1.1/gallery/statistics/boxplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# fake data\n",
    "np.random.seed(19680801)\n",
    "data = np.random.lognormal(size=(17, 4), mean=1.5, sigma=1.75)\n",
    "labels = list('ABCD')\n",
    "fs = 10  # fontsize\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(6, 6), sharey=True)\n",
    "axs[0, 0].boxplot(data, labels=labels)\n",
    "axs[0, 0].set_title('Default', fontsize=fs)\n",
    "\n",
    "axs[0, 1].boxplot(data, labels=labels, showmeans=True)\n",
    "axs[0, 1].set_title('showmeans=True', fontsize=fs)\n",
    "\n",
    "axs[0, 2].boxplot(data, labels=labels, showmeans=True, meanline=True)\n",
    "axs[0, 2].set_title('showmeans=True,\\nmeanline=True', fontsize=fs)\n",
    "\n",
    "axs[1, 0].boxplot(data, labels=labels, showbox=False, showcaps=False)\n",
    "tufte_title = 'Tufte Style \\n(showbox=False,\\nshowcaps=False)'\n",
    "axs[1, 0].set_title(tufte_title, fontsize=fs)\n",
    "\n",
    "axs[1, 1].boxplot(data, labels=labels, notch=True, bootstrap=10000)\n",
    "axs[1, 1].set_title('notch=True,\\nbootstrap=10000', fontsize=fs)\n",
    "\n",
    "axs[1, 2].boxplot(data, labels=labels, showfliers=False)\n",
    "axs[1, 2].set_title('showfliers=False', fontsize=fs)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10.3",
   "language": "python",
   "name": "python3103"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1d94489855e646012f575bcef68c6dcb5dcb0499f41a8ed39bb9ba537b7abb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
