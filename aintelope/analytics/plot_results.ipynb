{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for Aintelope\n",
    "Run these blocks for all tests, then scroll to the title you're interested in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import dateutil.parser as dparser\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import aintelope.training.dqn_training\n",
    "from aintelope.training.dqn_training import ReplayMemory\n",
    "import aintelope.agents\n",
    "from aintelope.agents import get_agent_class\n",
    "#from aintelope.agents.inference_agent import InferenceAgent\n",
    "from aintelope.environments.savanna_gym import SavannaGymEnv\n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hunger_20231127181536']\n"
     ]
    }
   ],
   "source": [
    "# Paths and directories\n",
    "outputs_dir = root_dir+'/outputs/' \n",
    "conf_dir = root_dir+'/aintelope/config/config_experiment.yaml'\n",
    "\n",
    "available_records = os.listdir(outputs_dir)\n",
    "dirs = [os.path.join(outputs_dir, f) for f in available_records] # add path to each file\n",
    "dirs.sort(key=lambda x: os.path.getmtime(x))\n",
    "latest_exp_dir = dirs[-1] #arbitrary use of latest\n",
    "\n",
    "print(available_records)\n",
    "\n",
    "#last_checkpoint = latest_exp_dir+\"/checkpoints/last.ckpt\"\n",
    "\n",
    "# Load a model for evaluation\n",
    "def load_checkpoint(PATH):\n",
    "    #https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "    model = Net()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(latest_exp_dir)\n",
    "print(dparser.parse(latest_exp_dir,fuzzy=True))\n",
    "df = pd.read_csv(latest_exp_dir+\"/memory_records.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "cfg = OmegaConf.load(conf_dir)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = load_checkpoint(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "keys = ([\"agent_coords\"] + \n",
    "        [f\"grass_patch_{i}\" for i in range(env.metadata[\"amount_grass_patches\"])] + \n",
    "        [f\"water_hole_{i}\" for i in range(env.metadata[\"amount_water_holes\"])])\n",
    "StateTuple = namedtuple(\"StateTuple\", {k: np.ndarray for k in keys})\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "event_x = []\n",
    "event_y = []\n",
    "event_type = []\n",
    "food_x = []\n",
    "food_y = []\n",
    "water_x = []\n",
    "water_y = []\n",
    "for _ ,row in df.iterrows():\n",
    "    \n",
    "    state = eval(row['state'])\n",
    "    #print(state)\n",
    "    x.append(state[0][0])\n",
    "    y.append(state[0][1])\n",
    "    \n",
    "    #refactor\n",
    "    food_x.append(state[1][0])\n",
    "    food_y.append(state[1][1])\n",
    "    #food_x.append(state[2][0])\n",
    "    #food_y.append(state[2][1])    \n",
    "\n",
    "    if row['instinct_events'] != '[]':\n",
    "        event_x.append(x[-1])\n",
    "        event_y.append(y[-1])\n",
    "        event_type.append(row['instinct_events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(data={'x':x, 'y':y})\n",
    "print(agent_df.head(), len(agent_df))\n",
    "\n",
    "food_df = pd.DataFrame(data={'x':food_x, 'y':food_y})\n",
    "print(food_df.head(), len(food_df))\n",
    "\n",
    "#water_df = pd.DataFrame(data={'x':water_x, 'y':water_y})\n",
    "#print(water_df.head(), len(water_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.DataFrame(data={'x':event_x, 'y':event_y, 'event_type':event_type})\n",
    "print(len(event_df))\n",
    "print(event_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_df['x'], agent_df['y'], '.r-')\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward received over time\n",
    "df['reward'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = 'autumn' # starts yellow, goes orange, then red\n",
    "n_points = len(agent_df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111) \n",
    "cm = plt.get_cmap(color_map)\n",
    "for i in range(10):\n",
    "    ax1.set_prop_cycle('color', cm(np.linspace(0, 1, n_points - 1, endpoint=False)))\n",
    "    for i in range(n_points - 1):\n",
    "        plt.plot(agent_df['x'][i:i+2], agent_df['y'][i:i+2])\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuemaps for actions\n",
    "Note, these maps are new ones and don't correlate with the above ones as we randomly regenerate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "cfg = OmegaConf.load(conf_dir)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = load_checkpoint(latest_exp_dir+\"/checkpoints/\") # FIXME was last.cktp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the agent into each square and ask for its values for each action, then add that direction into the map\n",
    "valuemap = np.zeros((env.metadata['map_max']+2,env.metadata['map_max']+2,4))\n",
    "agent = env.agents[0]\n",
    "\n",
    "ACTION_MAP = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]]) # This is a copy from savanna.py, should be an accessible param\n",
    "for x in range(0, env.metadata['map_max']):\n",
    "    for y in range(0, env.metadata['map_max']):\n",
    "        if (env.grass_patches == [x,y]).all(1).any():\n",
    "            continue\n",
    "        if (env.water_holes == [x,y]).all(1).any():\n",
    "            continue\n",
    "        env.set_agent_position(agent, np.array([x,y]))\n",
    "        observation = env.observe(agent)\n",
    "        #print(env.agent_states[agent])\n",
    "        action_vals = model(Tensor(observation)).detach().numpy()\n",
    "        offset = ACTION_MAP\n",
    "        for action in range(len(ACTION_MAP)):\n",
    "            x_ = offset[action][0]+x\n",
    "            y_ = offset[action][1]+y\n",
    "            valuemap[x_,y_,action] = action_vals[action]\n",
    "            \n",
    "valuemap = np.sum(valuemap,2)/len(ACTION_MAP)\n",
    "\n",
    "#print(valuemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(valuemap[1:-1,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render() isnt working atm\n",
    "maps = np.zeros((env.metadata['map_max'],env.metadata['map_max']))\n",
    "for grs in env.grass_patches:\n",
    "    print(grs[0])\n",
    "    maps[int(grs[0]),int(grs[1])] = 2.0\n",
    "for wtr in env.water_holes:\n",
    "    maps[int(wtr[0]),int(wtr[1])] = 4.0\n",
    "sns.heatmap(maps)\n",
    "# RED FOOD, LIGHT water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agent_position(agent, np.array([2,2]))\n",
    "observation = env.observe(agent)\n",
    "action_vals = model(Tensor(observation)).detach().numpy()\n",
    "print(action_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance plots\n",
    "These plots don't have the exploration bonus as confabulators (such as epsilon-greedy).\n",
    "Run the model N times, then change n_latest to this N and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testrun(model,env):\n",
    "    # Run each model for 10 different resets, with 10 different locations on the map.\n",
    "    # Gather the cumulative reward, -1 has to be given on each step though\n",
    "    # Or reset each time the food is found?\n",
    "    \n",
    "    action_space = env.action_space\n",
    "    observation, info = env.reset()\n",
    "    n_observations = len(observation)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        cfg, n_observations, action_space\n",
    "    ) \n",
    "    get_agent_class(cfg.hparams.agent_id)(\n",
    "        0,\n",
    "        trainer,\n",
    "        0,#cfg.hparams.warm_start_steps,\n",
    "        **cfg.hparams.agent_params,\n",
    "    )\n",
    "    epsilon = 0.0\n",
    "    device = \"cpu\"\n",
    "    start_pos = [[0,0],\n",
    "                [env.metadata['map_max'],env.metadata['map_max']],\n",
    "                [env.metadata['map_max'],0],\n",
    "                [0,env.metadata['map_max']]\n",
    "                ] #list of starting positions for agent, to test robustly each model\n",
    "    results = []\n",
    "    rewards = []\n",
    "    for j in range(len(start_pos)):\n",
    "        agent.reset()\n",
    "        env.set_agent_position(agent, np.array(start_pos[j]))\n",
    "        score = 0\n",
    "        for i in range(20):\n",
    "            reward, done = agent.play_step(model, epsilon, device)\n",
    "            rewards.append(reward)\n",
    "            score -= 1 # to make sure that longer play is penalized? TODO\n",
    "            if reward > 0:\n",
    "                break\n",
    "        results.append(score)\n",
    "    return results, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m env \u001b[39m=\u001b[39m SavannaGymEnv(env_params\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39menv_params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(models)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([\u001b[39mlen\u001b[39m(models),\u001b[39mlen\u001b[39m(models[\u001b[39m0\u001b[39;49m])])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m rewards \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joel/project/aintelope/aintelope/analytics/plot_results.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(models)): \u001b[39m# statistical significance\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Load all the models. Note that to make a statistical thing, run the training several times and then\n",
    "# take N-latest.\n",
    "n_latest = 5 # sample of n runs you've just 'make run-training'ed a moment ago\n",
    "latest_dirs = dirs[-n_latest:-1]\n",
    "\n",
    "# Read and sort models for testing\n",
    "models = []\n",
    "for exp_dir in latest_dirs:\n",
    "    mod_dir = os.listdir(exp_dir+\"/checkpoints/\")\n",
    "    runs_dir = [os.path.join(exp_dir+\"/checkpoints/\", m) for m in mod_dir]\n",
    "    runs_dir.sort(key=lambda x: os.path.getmtime(x))\n",
    "    models.append(runs_dir)\n",
    "\n",
    "cfg = OmegaConf.load(conf_dir)\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "\n",
    "results = np.zeros([len(models),len(models[0])])\n",
    "rewards = []\n",
    "for i in range(len(models)): # statistical significance\n",
    "    for j in range(len(models[0])): # number of epochs per run\n",
    "        model = load_checkpoint(models[i][j])\n",
    "        model.eval()\n",
    "        # run model\n",
    "        result, reward = testrun(model, env)\n",
    "        rewards.append(reward)\n",
    "        results[i,j] = sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/joel/project/aintelope/outputs/hunger_20231127181536']\n"
     ]
    }
   ],
   "source": [
    "#print(rewards)\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "print(results)\n",
    "print(results.mean(axis=1))\n",
    "print(results.var(axis=1))\n",
    "event_df = pd.DataFrame(data={'x':event_x, 'y':event_y})\n",
    "df['reward'].plot()\n",
    "# what are these rewards?\n",
    "# which way is the matrix?\n",
    "# you should do preset envs with static distances\n",
    "# check agent epsilon\n",
    "# add nosmell smell to agent somehow\n",
    "# figure out REWARD...\n",
    "# qagent 120 add reward for emotions\n",
    "#print(agent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = cfg.hparams\n",
    "print(hparams.agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from aintelope.agents.memory import ReplayBuffer, Experience\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
