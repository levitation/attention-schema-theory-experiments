import pytest
import numpy as np
import numpy.testing as npt
from gym.spaces import Discrete
from pettingzoo.test import (
    max_cycles_test,
    render_test,
    performance_benchmark,
)
from pettingzoo.test.parallel_test import parallel_api_test
from pettingzoo.test import api_test
from pettingzoo.test.seed_test import parallel_seed_test
from pettingzoo.utils import parallel_to_aec


from aintelope.environments import savanna_zoo as sut
from aintelope.environments.savanna import ACTION_MAP
from aintelope.environments.savanna_zoo import (
    SavannaZooParallelEnv,
    SavannaZooSequentialEnv,
)
from aintelope.environments.env_utils.distance import (
    vec_distance,
    distance_to_closest_item,
)


def test_pettingzoo_api_parallel():
    parallel_api_test(sut.SavannaZooParallelEnv(), num_cycles=1000)


def test_pettingzoo_api_sequential():
    # TODO: refactor these values out to a test-params file
    env_params = {
        "num_iters": 500,  # duration of the game
        "map_min": 0,
        "map_max": 100,
        "render_map_max": 100,
        "amount_agents": 1,  # for now only one agent
        "amount_grass_patches": 2,
        "amount_water_holes": 2,
    }
    parallel_env = SavannaZooParallelEnv(env_params=env_params)
    # TODO: Nathan was able to get the sequential-turn env to work, using this conversion, but not the parallel env. why??
    sequential_env = parallel_to_aec(parallel_env)
    api_test(sequential_env, num_cycles=10, verbose_progress=True)


def test_seed():
    parallel_seed_test(sut.SavannaZooParallelEnv, num_cycles=10, test_kept_state=True)


def test_agent_states():
    env = sut.SavannaZooParallelEnv()

    env.reset()
    assert isinstance(env.unwrapped.agent_states, dict)
    assert all(
        isinstance(agent_state, np.ndarray)
        for agent_state in env.unwrapped.agent_states.values()
    )
    assert all(
        agent_state.shape == (2,) for agent_state in env.unwrapped.agent_states.values()
    )


def test_reward_agent():
    env = sut.SavannaZooParallelEnv()
    env.reset()
    # single grass patch
    agent_pos = np.random.randint(env.metadata["map_min"], env.metadata["map_max"], 2)
    grass_patch = np.random.randint(env.metadata["map_min"], env.metadata["map_max"], 2)
    min_grass_distance = distance_to_closest_item(agent_pos, grass_patch)
    reward_single = sut.reward_agent(min_grass_distance)
    assert reward_single == 1 / (1 + vec_distance(grass_patch, agent_pos))

    # multiple grass patches
    grass_patches = np.random.randint(
        env.metadata["map_min"], env.metadata["map_max"], size=(10, 2)
    )
    min_grass_distance = distance_to_closest_item(agent_pos, grass_patches)
    reward_many = sut.reward_agent(min_grass_distance)
    grass_patch_closest = grass_patches[
        np.argmin(np.linalg.norm(np.subtract(grass_patches, agent_pos), axis=1))
    ]
    assert reward_many == 1 / (1 + vec_distance(grass_patch_closest, agent_pos))


def test_move_agent():
    env = sut.SavannaZooParallelEnv()
    env.reset()

    agent = env.possible_agents[0]
    agent_states = env.unwrapped.agent_states

    for _ in range(1000):
        prev_state = np.copy(agent_states[agent])
        action = env.action_space(agent).sample()
        agent_states[agent] = sut.move_agent(agent_states[agent], action)
        npt.assert_array_equal(
            np.clip(
                prev_state + ACTION_MAP[action],
                env.metadata["map_min"],
                env.metadata["map_max"],
            ),
            agent_states[agent],
        )
        assert (
            env.metadata["map_min"] <= agent_states[agent][0] <= env.metadata["map_max"]
        )
        assert (
            env.metadata["map_min"] <= agent_states[agent][1] <= env.metadata["map_max"]
        )
        assert agent_states[agent].dtype == sut.PositionFloat


def test_step_result():
    env = sut.SavannaZooParallelEnv()
    num_agents = len(env.possible_agents)
    assert num_agents, f"expected 1 agent, got: {num_agents}"
    env.reset()

    agent = env.possible_agents[0]
    action = {agent: env.action_space(agent).sample()}
    observations, rewards, dones, info = env.step(action)

    assert not dones[agent]
    assert isinstance(observations, dict), "observations is not a dict"
    assert isinstance(
        observations[agent], np.ndarray
    ), "observations of agent is not an array"
    assert isinstance(rewards, dict), "rewards is not a dict"
    assert isinstance(rewards[agent], np.float64), "reward of agent is not a float64"


def test_done_step():
    env = sut.SavannaZooParallelEnv()
    assert len(env.possible_agents) == 1
    env.reset()

    agent = env.possible_agents[0]
    for _ in range(env.metadata["num_iters"]):
        action = {agent: env.action_space(agent).sample()}
        _, _, dones, _ = env.step(action)

    assert dones[agent]
    with pytest.raises(ValueError):
        action = {agent: env.action_space(agent).sample()}
        env.step(action)


def test_agents():
    env = sut.SavannaZooParallelEnv()

    assert len(env.possible_agents) == env.metadata["amount_agents"]
    assert isinstance(env.possible_agents, list)
    assert isinstance(env.unwrapped.agent_name_mapping, dict)
    assert all(
        agent_name in env.unwrapped.agent_name_mapping
        for agent_name in env.possible_agents
    )


def test_action_spaces():
    env = sut.SavannaZooParallelEnv()

    for agent in env.possible_agents:
        assert isinstance(env.action_space(agent), Discrete)
        assert env.action_space(agent).n == 4


def test_max_cycles():
    # currently the environment does not accept parameters like max_cycles
    # max_cycles_test(sut.SavannaZooParallelEnv)
    pass


def test_render():
    # TODO: close method not implemented
    # render_test(sut.SavannaZooParallelEnv)
    pass


def test_performance_benchmark():
    # will print only timing to stdout; not shown per default
    # performance_benchmark(sut.SavannaZooParallelEnv())
    pass
